import copy
import csv
import glob
import logging
import os
import sys

from mongoengine import connect, DoesNotExist

from mecoshark.resultparser.mongomodels import Project, File, FileState, Package, PackageState, Clazz, ClazzState, \
    Component, ComponentState, Method, MethodState, CloneInstance


class SourcemeterParser(object):
    def __init__(self, output_path, input_path, url, revisionHash):
        self.output_path = output_path
        self.input_path = input_path
        self.url = url
        self.revisionHash = revisionHash
        self.logger = logging.getLogger("sourcemeter_parser")
        self.stored_functions = {}
        self.stored_packages = {}
        self.stored_classes = {}
        self.stored_components = {}
        self.stored_methods = {}
        self.stored_interfaces = {}
        self.stored_enums = {}
        self.stored_annotations = {}

        # find projectid
        try:
            self.projectid = Project.objects(url=url).get().id
        except DoesNotExist:
            self.logger.error("Project with the url %s does not exist in the database! Execute vcsSHARK first!" % url)
            sys.exit(1)

        # get list of files in input_path
        self.input_files = []
        for root, dirs, files in os.walk(self.input_path, topdown=True):
            for name in files:
                full_file_path = os.path.join(root, name).replace(self.input_path, "")

                # Filter out git directory
                if not full_file_path.startswith("/.git/"):
                    self.input_files.append(full_file_path)

        # get all stored files of the project
        self.stored_files = {}
        for file in File.objects(projectId=self.projectid):
            self.stored_files[file.path] = file.id

    def store_data(self):
        raise NotImplementedError('This method must be implemented by sub-classes!')

    def store_clone_data(self):
        self.logger.info("Parsing & storing clone data...")
        clone_class_csv_path = glob.glob(os.path.join(self.output_path, "*-CloneClass.csv"))[0]
        clone_instance_csv_path = glob.glob(os.path.join(self.output_path, "*-CloneInstance.csv"))[0]

        clone_classes = {}
        with open(clone_class_csv_path) as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                clone_classes[row['ID']] = self.sanitize_metrics_dictionary(copy.deepcopy(row))

        with open(clone_instance_csv_path) as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                metrics_dict = self.sanitize_metrics_dictionary(copy.deepcopy(row))
                long_name = self.sanitize_long_name(row['Path'])

                CloneInstance.objects(projectId=self.projectid, name=row['ID'], revisionHash=self.revisionHash)\
                    .upsert_one(projectId=self.projectid, name=row['ID'], revisionHash=self.revisionHash,
                                componentId=self.stored_components[row['Component']],
                                fileId=self.stored_files[long_name], cloneClass=row['Parent'],
                                cloneClassMetrics=clone_classes[row['Parent']], cloneInstanceMetrics=metrics_dict,
                                startLine=row['Line'], startColumn=row['Column'], endLine=row['EndLine'],
                                endColumn=row['EndColumn'])
        self.logger.info("Finished parsing & storing clone data!")

    def store_file_csv(self):
        file_csv_path = glob.glob(os.path.join(self.output_path, "*-File.csv"))[0]

        with open(file_csv_path) as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                # first, we need to match the longname to the path of the file in the mongodb
                long_name = self.sanitize_long_name(row['LongName'])
                if long_name is not None and long_name in self.stored_files:

                    metrics_dict = self.sanitize_metrics_dictionary(copy.deepcopy(row))

                    # save the file state to the gathered file id
                    FileState.objects(fileId=self.stored_files[long_name], revisionHash=self.revisionHash) \
                        .upsert_one(fileId=self.stored_files[long_name], revisionHash=self.revisionHash,
                                    metrics=metrics_dict)

                else:
                    # Sometime files get generated by makefiles, which are then part of the metrics, but are not
                    # listed in git (as they are never committed). Thats why we check here and continue on if we
                    # have such a file).
                    self.logger.debug("Problem in retrieving correct file "
                                      "id for file %s!" % row['LongName'])
                    continue

    def store_components_csv(self):
        components_csv_path = glob.glob(os.path.join(self.output_path, "*-Component.csv"))[0]

        with open(components_csv_path) as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                # first, we need to match the longname to the path of the file in the mongodb
                long_name = row['LongName']
                if long_name.startswith("/"):
                    long_name = self.sanitize_long_name(row['LongName'])

                component = Component.objects(projectId=self.projectid, longName=long_name) \
                    .upsert_one(projectId=self.projectid, longName=long_name)
                self.stored_components[row['ID']] = component.id

                metrics_dict = self.sanitize_metrics_dictionary(copy.deepcopy(row))

                ComponentState.objects(componentId=component.id, revisionHash=self.revisionHash) \
                    .upsert_one(componentId=component.id, revisionHash=self.revisionHash, metrics=metrics_dict)

    def get_component_ids(self, row_component_ids):
        # get list of objectids for all components in the csv file
        row_component_ids = row_component_ids.split(",")
        component_object_ids = []
        for row_component_id in row_component_ids:
            component_object_ids.append(self.stored_components[row_component_id.strip()])
        return component_object_ids

    def store_packages_csv(self):
        package_csv_path = glob.glob(os.path.join(self.output_path, "*-Package.csv"))[0]
        with open(package_csv_path) as csvfile:
            reader = csv.DictReader(csvfile)

            for row in reader:
                package = Package.objects(projectId=self.projectid, longName=row['LongName']) \
                    .upsert_one(projectId=self.projectid, name=row["Name"], longName=row['LongName'])
                self.stored_packages[row['ID']] = package.id

                metrics_dict = self.sanitize_metrics_dictionary(copy.deepcopy(row))

                if row['Parent'] and row['Parent'] in self.stored_packages:
                    PackageState.objects(packageId=package.id, revisionHash=self.revisionHash) \
                        .upsert_one(packageId=package.id, revisionHash=self.revisionHash, metrics=metrics_dict,
                                    componentId=self.stored_components[row['Component']],
                                    parentPackage=self.stored_packages[row['Parent']])
                else:
                    PackageState.objects(packageId=package.id, revisionHash=self.revisionHash) \
                        .upsert_one(packageId=package.id, revisionHash=self.revisionHash, metrics=metrics_dict,
                                    componentId=self.stored_components[row['Component']])

    def store_classes_csv(self):
        classes_csv_path = glob.glob(os.path.join(self.output_path, "*-Class.csv"))[0]
        with open(classes_csv_path) as csvfile:
            reader = csv.DictReader(csvfile)

            for row in reader:
                long_name = self.sanitize_long_name(row['Path'])

                clazz = Clazz.objects(projectId=self.projectid, fileId=self.stored_files[long_name],
                                      longName=row['LongName'])\
                    .upsert_one(projectId=self.projectid, fileId=self.stored_files[long_name], name=row['Name'],
                                longName=row['LongName'])
                self.stored_classes[row['ID']] = clazz.id

                metrics_dict = self.sanitize_metrics_dictionary(copy.deepcopy(row))

                ClazzState.objects(clazzId=clazz.id, revisionHash=self.revisionHash)\
                        .upsert_one(clazzId=clazz.id, revisionHash=self.revisionHash, moduleId=self.stored_modules[row['Parent']],
                                    startLine=row['Line'], endLine=row['EndLine'], startColumn=row['Column'],
                                    endColumn=row['EndColumn'], metrics=metrics_dict,
                                    componentIds=self.get_component_ids(row['Component']))

    def store_methods_csv(self):
        methods_csv_path = glob.glob(os.path.join(self.output_path, "*-Method.csv"))[0]
        with open(methods_csv_path) as csvfile:
            reader = csv.DictReader(csvfile)

            for row in reader:
                long_name = self.sanitize_long_name(row['Path'])
                method = Method.objects(projectId=self.projectid, fileId=self.stored_files[long_name],
                                        longName=row['LongName'])\
                    .upsert_one(projectId=self.projectid, fileId=self.stored_files[long_name], name=row['Name'],
                                longName=row['LongName'])
                self.stored_methods[row['ID']] = method.id

                metrics_dict = self.sanitize_metrics_dictionary(copy.deepcopy(row))
                MethodState.objects(methodId=method.id, revisionHash=self.revisionHash)\
                    .upsert_one(methodId=method.id, revisionHash=self.revisionHash, clazzId=self.stored_classes[row['Parent']],
                                startLine=row['Line'], endLine=row['EndLine'], startColumn=row['Column'],
                                endColumn=row['EndColumn'], metrics=metrics_dict,
                                componentIds=self.get_component_ids(row['Component']))


    def sanitize_metrics_dictionary(self, metrics):
        del metrics['Name']
        del metrics['ID']

        if 'LongName' in metrics:
            del metrics['LongName']

        if 'type' in metrics:
            del metrics['type']

        if 'sortKey' in metrics:
            del metrics['sortKey']

        if 'Line' in metrics:
            del metrics['Line']

        if 'Column' in metrics:
            del metrics['Column']

        if 'EndLine' in metrics:
            del metrics['EndLine']

        if 'EndColumn' in metrics:
            del metrics['EndColumn']

        if 'Component' in metrics:
            del metrics['Component']

        if 'Path' in metrics:
            del metrics['Path']

        if 'Parent' in metrics:
            del metrics['Parent']

        if 'WarningBlocker' in metrics:
            del metrics['WarningBlocker']

        if 'WarningCritical' in metrics:
            del metrics['WarningCritical']

        if 'WarningInfo' in metrics:
            del metrics['WarningInfo']

        if 'WarningMajor' in metrics:
            del metrics['WarningMajor']

        if 'WarningMinor' in metrics:
            del metrics['WarningMinor']

        for name, value in metrics.items():
            if not value:
                metrics[name] = float(0)
            else:
                try:
                    metrics[name] = float(value)
                except ValueError:
                    metrics[name] = value

        return metrics

    def sanitize_long_name(self, long_name):
        # special case: longname is <System>, so return it this way
        if '<System>' == long_name:
            return long_name

        if self.input_path in long_name:
            return long_name.replace(self.input_path + "/", "")
        elif self.output_path in long_name:
            return long_name.replace(self.output_path + "/", "")
        else:
            long_name = "/".join(long_name.strip("/").split('/')[1:])

            # if long_name is not an empty string
            if long_name:
                return self.get_fullpath(long_name)
            else:
                return None

    def get_fullpath(self, long_name):
        for file_name in self.input_files:
            if file_name.endswith(long_name):
                return file_name
        return None
